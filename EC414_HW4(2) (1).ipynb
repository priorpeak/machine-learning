{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EC414_HW4(2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_ih-R1fJ6VG"
      },
      "source": [
        "# Homework 4: Linear Binary Classification Methods\n",
        "by Junyu Liu and Brian Kulis\n",
        "\n",
        "**Due date**: March 3, Wednesday by 11:59pm\n",
        "\n",
        "**Late** due date: March 6, Saturday by 11:59pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBxGJCzSKovR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mxor3LzKpqj"
      },
      "source": [
        "To run and solve this assignment, you must have access to a working Jupyter Notebook installation. We recommend Google Colab. If you are already familiar with Jupyter and have your own installation, you may use it; however, you will have to tweak Colab-specific commands we've entered here (for example, file uploads).\n",
        "\n",
        "To use Google Colab:\n",
        "\n",
        "1. Download this `ipynb` file.\n",
        "2. Navigate to https://colab.research.google.com/ and select `Upload` in the pop-up window.\n",
        "3. Upload this file. It will then open in Colab.\n",
        "\n",
        "The below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find Week 1 discussion material useful.\n",
        "\n",
        "To run code in a cell or to render Markdown+LaTeX press Ctrl+Enter or \"`Run`\" button above. To edit any code or text cell, double-click on its content. Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. You can add cells via `+` sign at the top left corner.\n",
        "\n",
        "**Submission instructions:** please upload your completed solution file as well as a scan of any handwritten answers to Gradescope by the due date (see Schedule)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTtN4g49nWrB"
      },
      "source": [
        "This homework is scored out of 100.\n",
        "\n",
        "\n",
        "**Important:** unless otherwise specified, you should **NOT** use loops. This is not to say loops are always bad, but avoiding them should help you:\n",
        "1.   get more familiar with common language features and libraries\n",
        "2.   write more efficient code\n",
        "3.   \"think in higher dimensions\" (get more comfortable with vectors, matrices, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32VwlhsN9ny"
      },
      "source": [
        "## **Question 1:** Creating a Dataset (10 points)\n",
        "\n",
        "For this assignment, we will create a simple linearly-separable dataset for binary classification. We have provided you with the code to generate the feature vectors. Notice that one class has significantly more samples than the other.\n",
        "\n",
        "**Important:** Although this dataset has only 1 feature, **ALL** the code you write in this assignment should be able to run as intended with more features. The only exception is where you are producing plots. Many functions you need to write will be tested for compatibility with more features in question 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS0rlO3eOCZK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import default_rng"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw5AUAsV5MQr"
      },
      "source": [
        "# Do NOT change\n",
        "rng = default_rng(1)\n",
        "x1 = rng.uniform(-4, 2, 460)\n",
        "x2 = rng.normal(5, np.sqrt(3), 40)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7rbmFam-xWg"
      },
      "source": [
        "### Problem a. (4 points)\n",
        "We need to create appropriate labels for the two classes. x1 is the feature vectors of the negative class and x2 the positive class. Also produce a colored feature-label scatter plot (the negative class should be blue and the positive class should be red)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "1i19iWdkAVpY",
        "outputId": "66a0c630-23f8-4a18-9300-a2c78c539f10"
      },
      "source": [
        "# WRITE CODE HERE: \r\n",
        "y1 = np.ones(460)\r\n",
        "y1 = np.negative(y1)\r\n",
        "\r\n",
        "y2 = np.ones(40)\r\n",
        "\r\n",
        "plt.title('Positive and Negative Class Data')\r\n",
        "plt.scatter(x1,y1)\r\n",
        "plt.scatter(x2,y2)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f1e2f4e6dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRLTGhczArfW"
      },
      "source": [
        "### Problem b. (3 points)\n",
        "Combine the data into X and y, and then perform a 75-25 train-test split. Use 32 as the random_state.\n",
        "\n",
        "Hint: X has to be a 2D array, so you need to use reshape at some point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofFs0MR9Dp7z"
      },
      "source": [
        "# WRITE CODE HERE:\n",
        "X = np.append(x1,x2)\n",
        "X = np.reshape(X,(500,1))\n",
        "\n",
        "y = np.append(y1,y2)\n",
        "\n",
        "random_state=32\n",
        "test_size=0.25\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=32, test_size=.25)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlkYTs619A_"
      },
      "source": [
        "### Problem c. (3 points)\n",
        "To conveniently account for the bias term in later parts, we will also store the feature vectors in extended form (each feature vector appended with 1). Apply this to both X_train and X_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LFCgyVU3EKY"
      },
      "source": [
        "#WRITE CODE HERE:\n",
        "ones1 = np.ones(xtrain.shape)\n",
        "ones2 = np.ones(xtest.shape)\n",
        "\n",
        "Xtr_ext = np.append(xtrain, ones1, axis=1) # X_train, extended\n",
        "Xte_ext = np.append(xtest, ones2, axis=1) # X_test, extended"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf7z43OcK2ax"
      },
      "source": [
        "## **Question 2:** OLS Regression for Binary Classification (20 points)\r\n",
        "\r\n",
        "In class, we talked about one way to implement clasification is to use one of the linear regression methods we learned. We will investigate how to use OLS for binary classification and see how it performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZX2J8wLVix"
      },
      "source": [
        "### Problem a. (3 points)\n",
        "\n",
        "When OLS takes a feature vector, it gives you a real-valued scalar. Assuming OLS works well (i.e., it gives a value close to the binary label (either -1 or 1) for most of the samples), how can you translate the value into the label? Suggest a simple method.\n",
        "\n",
        "Hint: we talked about this in class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gdm7yHm72aa"
      },
      "source": [
        "We could map the distance from the real-valued scalar to both 1 and -1. Whichever distance is smaller is the binary label that it should be mapped to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEFD30vtfu3T"
      },
      "source": [
        "### Problem b. (7 points)\n",
        "For simplicity, we will use the OLS implementation from sklearn whose documentation can be found here:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "\n",
        "Fit the model to X_train, y_train and then extract w and b. You should do so by using get_wOLS_ext, which you need to complete. Note that the function should return a single vector, which is the regular w vector with the bias term appended to it.\n",
        "\n",
        "Lastly, create the following plot:\n",
        "* a scatter plot of the training data overlayed by the OLS solution. For this, You can assume that X_train has only one feature.\n",
        "\n",
        "Hint: w is the coeficient and b is the intercept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvgEoJUMcpEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa653e37-773a-443f-badc-1467be9e18ea"
      },
      "source": [
        "from sklearn import linear_model\r\n",
        "# WRITE CODE HERE: \r\n",
        "# DO NOT assume that X has only one feature\r\n",
        "def get_wOLS_ext(X, y):\r\n",
        "    model = linear_model.LinearRegression().fit(X,y)\r\n",
        "    w_ols_sklearn = np.append(model.coef_[:-1], model.intercept_)\r\n",
        "    return w_ols_sklearn\r\n",
        "\r\n",
        "wOLS_ext = get_wOLS_ext(Xtr_ext, ytrain)\r\n",
        "\r\n",
        "print(wOLS_ext)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.15042363 -0.76859499]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhgVuFW9w773"
      },
      "source": [
        "### Problem c. (6 points)\n",
        "\n",
        "Complete the linear_binary_predict function to predict the labels of the test set using the OLS solution from part a. Also complete compute_CCR and report the correct classification rate (CCR).\n",
        "\n",
        "linear_binary_predict takes Xext, a matrix of extended feature vectors, and wext, an extended weight vector; it performs linear binary classification and returns a vector of predicted binary labels.\n",
        "\n",
        "compute_CCR takes Xext, wext, along with y, the expected binary labels to compute CCR. It should make use of linear_binary_predict.\n",
        "\n",
        "Note: \n",
        "*   We assume Xext to be row-major, meaning each row is a sample.\n",
        "*   Do not change the function prototype of linear_binary_predict. This also applies to all other functions we ask you to complete\n",
        "*   The OLS from sklearn has a builtin predict function. We are asking you to complete linear_binary_predict instead of using the builtin predict because linear_binary_predict will be important for later parts.\n",
        "\n",
        "Hint: use the idea from part a.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrhLQ-0_x8OX",
        "outputId": "40c05f32-e194-4faa-9e21-b086cdc3d539"
      },
      "source": [
        "# WRITE CODE HERE:\n",
        "def linear_binary_predict(Xext, wext):\n",
        "    Xext = Xext @ wext\n",
        "    label_predict = np.sign(Xext)\n",
        "    return label_predict\n",
        "\n",
        "def compute_CCR(Xext, wext, y):\n",
        "    predict = linear_binary_predict(Xext, wext)\n",
        "    predict = predict - y\n",
        "    correct_predictions = np.count_nonzero(predict == 0)\n",
        "    CCR = correct_predictions / len(predict)\n",
        "    return CCR\n",
        "\n",
        "CCR = compute_CCR(Xte_ext, wOLS_ext, ytest)\n",
        "print(\"The test CCR using OLS is\", CCR)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using OLS is 0.928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX-NCA9sZ5XO"
      },
      "source": [
        "### Problem d. (4 points)\n",
        "Explain why CCR is not a good metric in this case. Suggest an alternative that better captures the performance of OLS on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZmVPImh-XSk"
      },
      "source": [
        "CCR is not a good metric in this case because the datasets are unbalanced. X1 is a much larger dataset than X2. An alternative metric would be to compare the output of a Loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF7u7IoGC96w"
      },
      "source": [
        "## **Question 3:** Fisher's Linear Discriminant (27 points)\n",
        "Fisher's Linear Discriminant is a method that takes d dimensional feature vectors and projects them into 1 dimension, and it tries to do so in a fashion where the resulting 1D values are well-separated by class. Here our d happens to be 1, but as mentioned above, the code you write needs to be able to accomodate higher dimensions for full credit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaVLlwwJWrpX"
      },
      "source": [
        "### a. Separate the training set by class (4 points)\n",
        "\n",
        "Write a function seperate that takes the inputs X, y and separates X based on y.\n",
        "\n",
        "For full credit, create a vectorized implementation (no for loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "991ZOOYCY02l"
      },
      "source": [
        "def seperate(X, y):\n",
        "    # WRITE CODE HERE:\n",
        "    X1 = X[y <= 0]\n",
        "    X2 = X[y >= 0]\n",
        "    # X1 should be of the negative class, and X2 the positive class\n",
        "    return X1, X2\n",
        "\n",
        "X1, X2 = seperate(xtrain, ytrain)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMMqIr0xB9BP"
      },
      "source": [
        "### b. Calculate mean vectors (4 points)\n",
        "Write a function get_means that takes the inputs X1, X2 and calculates the mean vectors of the two classes.\n",
        "\n",
        "For full credit, create a vectorized implementation (no for loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuPgvLQKTPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8d481c-87b7-48b6-b091-29f345345b8c"
      },
      "source": [
        "def get_means(X1, X2):\n",
        "    # WRITE CODE HERE:\n",
        "    m1 = np.mean(X1, axis=0)\n",
        "    m2 = np.mean(X2, axis=0)\n",
        "    return m1, m2 \n",
        "\n",
        "m1, m2 = get_means(X1, X2)\n",
        "print(m1)\n",
        "print(m2)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.07085432]\n",
            "[5.73185771]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3xQqSq_Ogmd"
      },
      "source": [
        "### c. Calculate within-class covariance (5 points)\n",
        "$S_w$, the total within-class covariance matrix, is a d by d matrix given by $S_1 + S_2$ where $S_1 = \\sum_{x_i \\in X_1}(x_i-m_1)^T(x_i-m_1) = (X_1-m_1)^T(X_1-m_1)$. \n",
        "\n",
        "Write a function get_Sw that takes the inputs X1, X2, m1, m2 and calculates Sw.\n",
        "\n",
        "For full credit, create a vectorized implementation.\n",
        "\n",
        "**Note:** depending on the schema of the dataset (row-major vs column-major), the formula to use might differ slightly from those on the lecture slides. We assume row-major (each sample is a row) here, as many public datasets are organized this way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5NaX0uwUXll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c15e7a-467b-4f7e-d831-dbe978d9cccb"
      },
      "source": [
        "def get_Sw(X1, X2, m1, m2):\n",
        "    # WRITE CODE HERE:\n",
        "    S1 = X1 - m1\n",
        "    S2 = X2 - m2\n",
        "    Sw = np.transpose(S1) @ S1 + np.transpose(S2) @ S2\n",
        "    return Sw\n",
        "\n",
        "Sw = get_Sw(X1, X2, m1, m2)\n",
        "print(Sw)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1106.73594709]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnV8WWDBdZAG"
      },
      "source": [
        "### d. Calculate $w_{FLD}$ and $b_{FLD}$ (5 points)\n",
        "Write a function get_wFLD_ext that takes the inputs Sw, m1, m2 and calculates the extended $w_{FLD}$.\n",
        "\n",
        "Recall that in class we mentioned the average of the averages of the two classes after the linear transformation can serve as the bias term. However, since we want the outputs of $Xw + b$ on the two classes to be roughly separated by 0, we actually want the effect of adding b to be the same as subtracting that average. Thus, use $b = -(m_1w + m_2w)/2$.\n",
        "\n",
        "For full credit, create a vectorized implementation, and do not use the inverse function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1BuMyNvdXm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180e6890-16ae-4e75-b557-2f8397169965"
      },
      "source": [
        "def get_wFLD_ext(Sw, m1, m2):\n",
        "    # WRITE CODE HERE: \n",
        "    w = np.linalg.solve(Sw, m2 - m1)\n",
        "    b = -(m1 @ w + m2 @ w) / 2\n",
        "    return np.append(w, b)\n",
        "\n",
        "wFLD_ext = get_wFLD_ext(Sw, m1, m2)\n",
        "print('wFLD_ext:', wFLD_ext)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wFLD_ext: [ 0.00614664 -0.01432476]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5FyLF32e9t0"
      },
      "source": [
        "### e. Evaluation (4 points)\n",
        "\n",
        "First, create a plot of the FLD solution using the training set. Then use the compute_CCR function that you wrote earlier to get the FLD CCR on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9hWd2qTM_xun",
        "outputId": "12ed6325-fe75-432f-d0fe-14702ef0737a"
      },
      "source": [
        "# WRITE CODE HERE: \n",
        "plt.plot(xtrain[0:,0], np.transpose(xtrain[0:,0] * wFLD_ext[0] + wFLD_ext[1]))\n",
        "CCR = compute_CCR(Xte_ext, wFLD_ext, ytest)\n",
        "print(\"The test CCR using FLD is\", CCR)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using FLD is 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kG8hSp7WHCD"
      },
      "source": [
        "### f. Reflection (5 points)\n",
        "Recall that in class we noted that OLS and FLD can be equivalent under certain situations. Is that the case here? If not, briefly comment on why they performed differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa2X0WzPCFKw"
      },
      "source": [
        "OLS and FLD are not equivalent in this case because the datasets are so unbalanced. Since X1 is so much larger than X2, OLS and FLD are unlikely to give us equivalent results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hte8xdEpZNZ"
      },
      "source": [
        "## **Question 4:** An algorithm using SGD (33 points)\n",
        "Let us consider a different linear classifier in this problem, one based on the idea of an $\\textit{error}$ function minimization problem, similar to what we studied with linear regression, but whose error function is more tailored to the classification problem.\n",
        "\n",
        "The form of our linear classifier is $h_{\\bf{w}}(\\bf{x}) = \\mbox{sgn}(\\bf{w}^T \\bf{x})$, where $sgn$ is the sign function (so $1$ if $\\bf{w}^T \\bf{x} \\geq 0$ and $-1$ otherwise).  We are assuming here that we have padded the inputs with an extra dimension of $1$ to account for the bias term in the linear function, as we did with linear regression.\n",
        "\n",
        "In the binary classification case, each target class $y_i$ is either $1$ or $-1$.  One possible error function to use would say that the error is 0 if our linear classifier agrees with the target label (i.e. $y_i = \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), and if our linear classifier predicts the incorrect class ($y_i \\neq \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), then the error will be given by $|\\bf{w}^T \\bf{x}_i|$---intuitively, we penalize more for predictions that are farther from being predicted correctly.\n",
        "\n",
        "**Note:** the analytical parts of this question assume column feature vectors. However, you should still assume row feature vectors in the coding parts for consistency with our datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19cL4fQjsHHC"
      },
      "source": [
        "### Problem a. (5 points)\n",
        "Show that, for each training point, this error function may be concisely written as\n",
        "\n",
        "$L_i(\\bf{w}) = \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJyt3ZrBuG2"
      },
      "source": [
        "If w is computed correctly, it will result in a positive term for the second parameter of the max function. This means that 0 will always be the greater value, so our Loss will ideally be 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlqWT0CXs5vk"
      },
      "source": [
        "### Problem b. (5 points)\n",
        "We will compute the total loss over the training data as the sum of the $L_i$ losses, namely\n",
        "\n",
        "$L(\\bf{w}) = \\sum_{i=1}^n \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$.\n",
        "\n",
        "Note that one reason we chose this particular loss function is that it is continuous (unlike the 0-1 loss).  Furthermore, it has a simple sub-gradient.  The sub-gradient of the loss at 0 is 0, and everywhere else it is equal to the gradient.  Show that the sub-gradient is equal to\n",
        "$\\nabla_{\\bf{w}} L_i = \n",
        "\\begin{cases}\n",
        "0 & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ correctly}\\\\\n",
        "-y_i \\bf{x}_i & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ incorrectly.}\n",
        "\\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQ26ILVB8xY"
      },
      "source": [
        "(See bottom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF24kNfQuSKB"
      },
      "source": [
        "### Problem c. (5 points)\n",
        "Using a batch size of 1 and a step size of 1, write down the update rule for stochastic (sub)-gradient descent to minimize $L(\\bf{w})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BZZldGACCr_"
      },
      "source": [
        "(See bottom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpOT7Z6BrV6T"
      },
      "source": [
        "### Problem d. (12 points)\n",
        "Now it's time to implement the actual algorithm using the update rule we derived from part c. \n",
        "\n",
        "1.   Complete the one_pass function, which iterates over the entire Xext and y once, updating w in the process. (6 points)\n",
        "2.   Go over the training set twice (2 epochs) to get your final w, which you can do by calling one_pass twice in the get_wSGD function. Then report the CCR on the test set. (6 points)\n",
        "\n",
        "Note:\n",
        "* You should always shuffle the training set before each pass.\n",
        "* The first shuffle has already been performed by train_test_split, so you do not need to do a shuffle before the first pass here.\n",
        "* You can get rid of the one_pass function if you think it's better to incorporate it into get_wSGD, but you must keep get_wSGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14KZEtjvOuO",
        "outputId": "710afd48-d376-488b-dacb-a10680aef067"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def one_pass(Xext, y, w):\n",
        "    n = Xext.shape[0]\n",
        "    for i in range(n):\n",
        "        # WRITE CODE HERE:\n",
        "        if y[i] == np.sign(np.transpose(w) @ Xext[i]):\n",
        "          w = w\n",
        "        else:\n",
        "          w = w + y[i] * Xext[i]\n",
        "    return w\n",
        "def get_wSGD(Xext, y):\n",
        "    # use 0 as the random state for shuffling\n",
        "    random_state=0\n",
        "    # WRITE CODE HERE:\n",
        "    w = np.zeros(Xext.shape[1])\n",
        "    w = one_pass(Xext, y, w)\n",
        "    Xext, y = shuffle(Xext, y, random_state=0)\n",
        "    w = one_pass(Xext, y, w)\n",
        "    return w\n",
        "\n",
        "wSGD = get_wSGD(Xtr_ext, ytrain)\n",
        "CCR = compute_CCR(Xte_ext, wSGD, ytest)\n",
        "print(\"The test CCR using SGD is\", CCR)\n",
        "print('wSGD:', wSGD)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using SGD is 0.92\n",
            "wSGD: [ 4.48944846 -5.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAyX-08Z1f5h"
      },
      "source": [
        "### Problem e. (6 points)\n",
        "\n",
        "The result you get from the previous part is likely not ideal. And if you change the random_state used for shuffling in the previous part (remember to change it back to 0 if you try this), you should see large swings in CCR. This is a common phenomenon when using SGD naively, and we will see this again in question 5.\n",
        "\n",
        "Recall from part c that we are using a fixed step size of 1, which is way too large for you to converge to a local minimum consistently. \n",
        "\n",
        "In practice, a technique called learning rate decay is commonly used with SGD. Essentially, the step size gets smaller after each iteration. This allows you to \"learn fast\" in the beginning and also be able to converge to a local minimum later when you are close to one.\n",
        "\n",
        "We will use an initial learning rate (step size) of 1 and the inverse square root decay, which means the step size at iteration $t$ is $\\frac1{\\sqrt t}$.\n",
        "\n",
        "Note:\n",
        "* $t$ starts at 1, not 0; otherwise, you will get a division by 0\n",
        "* You can get rid of the one_pass2 function if you think it's better to incorporate it into get_wSGD2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlM2SdxOvGvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b893f10c-0fa0-448f-a29e-8a50429788ed"
      },
      "source": [
        "import math\n",
        "\n",
        "def one_pass2(Xext, y, w, t):\n",
        "    n = Xext.shape[0]\n",
        "    iteration = t\n",
        "    for i in range(n):\n",
        "        # WRITE CODE HERE:\n",
        "        if y[i] == np.sign(np.transpose(w) @ Xext[i]):\n",
        "          w = w\n",
        "        else:\n",
        "          w = w + (1 / math.sqrt(iteration)) * y[i] * Xext[i]\n",
        "\n",
        "        iteration += 1\n",
        "    return w\n",
        "def get_wSGD2(Xext, y):\n",
        "    # use 0 as the random state for shuffling\n",
        "    random_state=0\n",
        "    # WRITE CODE HERE:\n",
        "    w = np.zeros(Xext.shape[1])\n",
        "    w = one_pass2(Xext, y, w, 1)\n",
        "    Xext, y = shuffle(Xext, y, random_state=0)\n",
        "    w = one_pass2(Xext, y, w, 1)\n",
        "    return w\n",
        "\n",
        "wSGD2 = get_wSGD2(Xtr_ext, ytrain)\n",
        "CCR = compute_CCR(Xte_ext, wSGD2, ytest)\n",
        "print(\"The test CCR using SGD with decay is\", CCR)\n",
        "print('wSGD2:', wSGD2)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using SGD with decay is 1.0\n",
            "wSGD2: [ 0.92169208 -2.0213996 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcnj3HYJHTqa"
      },
      "source": [
        "## **Question 5:** Testing the above methods on a real-world dataset (10 points)\n",
        "No code is required for this part. Getting part a to run correctly is not required for part b.\n",
        "\n",
        "The purpose of this question is to compare the three (four if you count SGD with decay as a separate one) methods on a real-world dataset. We will use the famous Iris dataset created by Sir Ronald Fisher (the same Fisher as in Fisher's Linear Discriminant). It contains feature measurements of 150 samples from three *Iris* flower species. We will combine two of the classes into a single negative class and the remaining class is the positive class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekpBYEcfB8-3"
      },
      "source": [
        "### Problem a. (6 points)\n",
        "Run the cells below. The code for this part is provided to you, but it requires several earlier functions to be properly implemented to run. They need to be able to handle data with multiple features.\n",
        "\n",
        "You will be graded on how well the behavior of this part matches our expectation. Do NOT modify the code provided here; if you have issues running it, check the code you wrote in previous parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usHZVolZXXen"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "# Split data into feature vectors and labels\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# change the label values for binary classification\n",
        "y[:100] = -1\n",
        "y[100:] = 1"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yfPgky0OvO-"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234, test_size=0.25)\n",
        "Xtr_ext = np.c_[X_train, np.ones(X_train.shape[0])]\n",
        "Xte_ext = np.c_[X_test, np.ones(X_test.shape[0])]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9coGwIiKQZPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13f2397-b45a-4cd3-c459-e95bbf9291ae"
      },
      "source": [
        "wOLS = get_wOLS_ext(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wOLS, y_test)\n",
        "print(\"The test CCR using OLS is\", CCR)\n",
        "\n",
        "X1, X2 = seperate(X_train, y_train)\n",
        "m1, m2 = get_means(X1, X2)\n",
        "Sw = get_Sw(X1, X2, m1, m2)\n",
        "wFLD_ext = get_wFLD_ext(Sw, m1, m2)\n",
        "CCR = compute_CCR(Xte_ext, wFLD_ext, y_test)\n",
        "print(\"The test CCR using FLD is\", CCR)\n",
        "\n",
        "wSGD = get_wSGD(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD, y_test)\n",
        "print(\"The test CCR using SGD is\", CCR)\n",
        "\n",
        "wSGD2 = get_wSGD2(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD2, y_test)\n",
        "print(\"The test CCR using SGD with decay is\", CCR)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using OLS is 0.8947368421052632\n",
            "The test CCR using FLD is 0.8947368421052632\n",
            "The test CCR using SGD is 0.6052631578947368\n",
            "The test CCR using SGD with decay is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxpckb9mCE_P"
      },
      "source": [
        "### Problem b. (4 points)\n",
        "A class containing sub-classes is quite common in real-world classifications. One direct effect of this is that within such a class, the feature vectors tend to be quite spread out (in distinct clusters). Comment on the effect this may have on OLS and FLD.\n",
        "\n",
        "Hint: If the previous part runs correctly, you should see OLS and FLD perform worse than the SGD algorithm from problem 4e. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71oQW2IFCt7"
      },
      "source": [
        "If the feature vectors are more spread out (or data is in higher dimensions), it's more difficult for OLS and FLD to find an accurate weight vector that fits the data in higher dimensions, while SGD is much more capable because it finds the absolute minimum of the derivative of the Loss function with respect to W."
      ]
    }
  ]
}